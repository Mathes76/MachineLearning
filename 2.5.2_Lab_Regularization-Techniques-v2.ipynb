{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4165dc31-9797-4445-b04c-a0052ab922cb",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"300\" alt=\"cognitiveclass.ai logo\">\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b6f089-9348-4344-bb85-101498ea776d",
   "metadata": {},
   "source": [
    "# Regularization Techniques\n",
    "\n",
    "\n",
    "Estimated time needed: **60** minutes\n",
    "\n",
    "The purpose of Regularization techniques is to reduce the degree of overfitting that can occur in Regression models. Overfitting leads to poor ability of the model to make predictions on the new, unseen data. As we saw in the previous Regression Lessons, with a creation of extra features, such as through polynomial regression, a model can become easily overfit. To reduce the overfitting, we can regularize the model, or in other words, we can decrease its degrees of freedom. A simple way to regularize polynomial model is to reduce the number of polynomial degrees. For a linear regression model, regularization is typically achieved by constraining the weights of the model. Regularizer imposes a penalty on the size of the coefficients of the model.\n",
    "\n",
    "In this lab, we will cover three types of regularizers:\n",
    "\n",
    "*   Ridge regression\n",
    "*   Lasso regression\n",
    "*   Elastic Net\n",
    "\n",
    "Each one has its own advantages and disadvantages. Lasso will eliminate many features and reduce overfitting in your linear model. Ridge will reduce the impact of the features that are not important in predicting your target. Elastic Net combines feature elimination from Lasso and feature coefficient reduction from the Ridge model to improve your modelâ€™s predictions.\n",
    "\n",
    "The common features of all these regularizers include using cross-validation to select hyperparameters and applying data normalization to improve the performance.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "After completing this lab you will be able to:\n",
    "\n",
    "*   Understand the advantages and disadvantages of Ridge, Lasso and Elastic Net Regressions\n",
    "*   Apply Ridge, Lasso and Elastic Net Regressions\n",
    "*   Perform  hyperparameters Grid Search on a model using validation data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cec20b0-e82d-4aa4-811b-5e94b575f9e8",
   "metadata": {},
   "source": [
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00546eff-a023-43c8-99e5-0221390cac1d",
   "metadata": {},
   "source": [
    "## **Setup**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bde5a01-99c8-4bef-aabb-40b112183120",
   "metadata": {},
   "source": [
    "For this lab, we will be using the following libraries:\n",
    "\n",
    "*   [`pandas`](https://pandas.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML240ENSkillsNetwork34171862-2022-01-01) for managing the data.\n",
    "*   [`numpy`](https://numpy.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML240ENSkillsNetwork34171862-2022-01-01) for mathematical operations.\n",
    "*   [`seaborn`](https://seaborn.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML240ENSkillsNetwork34171862-2022-01-01) for visualizing the data.\n",
    "*   [`matplotlib`](https://matplotlib.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML240ENSkillsNetwork34171862-2022-01-01) for visualizing the data.\n",
    "*   [`sklearn`](https://scikit-learn.org/stable/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML240ENSkillsNetwork34171862-2022-01-01) for machine learning and machine-learning-pipeline related functions.\n",
    "*   [`scipy`](https://docs.scipy.org/doc/scipy/tutorial/stats.html/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML240ENSkillsNetwork34171862-2022-01-01) for statistical computations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07da3353-c576-434e-82bb-e83a3de255bd",
   "metadata": {},
   "source": [
    "## **Import the required libraries**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a3423d0-741b-48a6-9a3d-335a14f3f1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required libraries\n",
    "# !pip install -U scikit-learn\n",
    "# !pip install pandas\n",
    "# !pip install numpy\n",
    "# !pip install seaborn\n",
    "# !pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e026902-e5f4-4ef0-ae41-7d645919f6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Surpress warnings:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2d805dd-3a1e-41dd-b93e-26f0dc580185",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m \n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m \n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mattes\\IBM\\MachineLearning\\.venv\\Lib\\site-packages\\pandas\\__init__.py:62\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# let init-time option registration happen\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig_init\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     63\u001b[39m     \u001b[38;5;66;03m# dtype\u001b[39;00m\n\u001b[32m     64\u001b[39m     ArrowDtype,\n\u001b[32m     65\u001b[39m     Int8Dtype,\n\u001b[32m     66\u001b[39m     Int16Dtype,\n\u001b[32m     67\u001b[39m     Int32Dtype,\n\u001b[32m     68\u001b[39m     Int64Dtype,\n\u001b[32m     69\u001b[39m     UInt8Dtype,\n\u001b[32m     70\u001b[39m     UInt16Dtype,\n\u001b[32m     71\u001b[39m     UInt32Dtype,\n\u001b[32m     72\u001b[39m     UInt64Dtype,\n\u001b[32m     73\u001b[39m     Float32Dtype,\n\u001b[32m     74\u001b[39m     Float64Dtype,\n\u001b[32m     75\u001b[39m     CategoricalDtype,\n\u001b[32m     76\u001b[39m     PeriodDtype,\n\u001b[32m     77\u001b[39m     IntervalDtype,\n\u001b[32m     78\u001b[39m     DatetimeTZDtype,\n\u001b[32m     79\u001b[39m     StringDtype,\n\u001b[32m     80\u001b[39m     BooleanDtype,\n\u001b[32m     81\u001b[39m     \u001b[38;5;66;03m# missing\u001b[39;00m\n\u001b[32m     82\u001b[39m     NA,\n\u001b[32m     83\u001b[39m     isna,\n\u001b[32m     84\u001b[39m     isnull,\n\u001b[32m     85\u001b[39m     notna,\n\u001b[32m     86\u001b[39m     notnull,\n\u001b[32m     87\u001b[39m     \u001b[38;5;66;03m# indexes\u001b[39;00m\n\u001b[32m     88\u001b[39m     Index,\n\u001b[32m     89\u001b[39m     CategoricalIndex,\n\u001b[32m     90\u001b[39m     RangeIndex,\n\u001b[32m     91\u001b[39m     MultiIndex,\n\u001b[32m     92\u001b[39m     IntervalIndex,\n\u001b[32m     93\u001b[39m     TimedeltaIndex,\n\u001b[32m     94\u001b[39m     DatetimeIndex,\n\u001b[32m     95\u001b[39m     PeriodIndex,\n\u001b[32m     96\u001b[39m     IndexSlice,\n\u001b[32m     97\u001b[39m     \u001b[38;5;66;03m# tseries\u001b[39;00m\n\u001b[32m     98\u001b[39m     NaT,\n\u001b[32m     99\u001b[39m     Period,\n\u001b[32m    100\u001b[39m     period_range,\n\u001b[32m    101\u001b[39m     Timedelta,\n\u001b[32m    102\u001b[39m     timedelta_range,\n\u001b[32m    103\u001b[39m     Timestamp,\n\u001b[32m    104\u001b[39m     date_range,\n\u001b[32m    105\u001b[39m     bdate_range,\n\u001b[32m    106\u001b[39m     Interval,\n\u001b[32m    107\u001b[39m     interval_range,\n\u001b[32m    108\u001b[39m     DateOffset,\n\u001b[32m    109\u001b[39m     \u001b[38;5;66;03m# conversion\u001b[39;00m\n\u001b[32m    110\u001b[39m     to_numeric,\n\u001b[32m    111\u001b[39m     to_datetime,\n\u001b[32m    112\u001b[39m     to_timedelta,\n\u001b[32m    113\u001b[39m     \u001b[38;5;66;03m# misc\u001b[39;00m\n\u001b[32m    114\u001b[39m     Flags,\n\u001b[32m    115\u001b[39m     Grouper,\n\u001b[32m    116\u001b[39m     factorize,\n\u001b[32m    117\u001b[39m     unique,\n\u001b[32m    118\u001b[39m     value_counts,\n\u001b[32m    119\u001b[39m     NamedAgg,\n\u001b[32m    120\u001b[39m     array,\n\u001b[32m    121\u001b[39m     Categorical,\n\u001b[32m    122\u001b[39m     set_eng_float_format,\n\u001b[32m    123\u001b[39m     Series,\n\u001b[32m    124\u001b[39m     DataFrame,\n\u001b[32m    125\u001b[39m )\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdtypes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparseDtype\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtseries\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m infer_freq\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mattes\\IBM\\MachineLearning\\.venv\\Lib\\site-packages\\pandas\\core\\api.py:47\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconstruction\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m array\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mflags\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Flags\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgroupby\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     48\u001b[39m     Grouper,\n\u001b[32m     49\u001b[39m     NamedAgg,\n\u001b[32m     50\u001b[39m )\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mindexes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     52\u001b[39m     CategoricalIndex,\n\u001b[32m     53\u001b[39m     DatetimeIndex,\n\u001b[32m   (...)\u001b[39m\u001b[32m     59\u001b[39m     TimedeltaIndex,\n\u001b[32m     60\u001b[39m )\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mindexes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatetimes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     62\u001b[39m     bdate_range,\n\u001b[32m     63\u001b[39m     date_range,\n\u001b[32m     64\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mattes\\IBM\\MachineLearning\\.venv\\Lib\\site-packages\\pandas\\core\\groupby\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgroupby\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgeneric\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      2\u001b[39m     DataFrameGroupBy,\n\u001b[32m      3\u001b[39m     NamedAgg,\n\u001b[32m      4\u001b[39m     SeriesGroupBy,\n\u001b[32m      5\u001b[39m )\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgroupby\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgroupby\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GroupBy\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgroupby\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgrouper\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Grouper\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mattes\\IBM\\MachineLearning\\.venv\\Lib\\site-packages\\pandas\\core\\groupby\\generic.py:73\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mframe\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataFrame\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgroupby\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     70\u001b[39m     base,\n\u001b[32m     71\u001b[39m     ops,\n\u001b[32m     72\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgroupby\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgroupby\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     74\u001b[39m     GroupBy,\n\u001b[32m     75\u001b[39m     GroupByPlot,\n\u001b[32m     76\u001b[39m     _agg_template_frame,\n\u001b[32m     77\u001b[39m     _agg_template_series,\n\u001b[32m     78\u001b[39m     _apply_docs,\n\u001b[32m     79\u001b[39m     _transform_template,\n\u001b[32m     80\u001b[39m )\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mindexes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     82\u001b[39m     Index,\n\u001b[32m     83\u001b[39m     MultiIndex,\n\u001b[32m     84\u001b[39m     all_indexes_same,\n\u001b[32m     85\u001b[39m     default_index,\n\u001b[32m     86\u001b[39m )\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mseries\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Series\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mattes\\IBM\\MachineLearning\\.venv\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:130\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgroupby\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    125\u001b[39m     base,\n\u001b[32m    126\u001b[39m     numba_,\n\u001b[32m    127\u001b[39m     ops,\n\u001b[32m    128\u001b[39m )\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgroupby\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgrouper\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_grouper\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgroupby\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mindexing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    131\u001b[39m     GroupByIndexingMixin,\n\u001b[32m    132\u001b[39m     GroupByNthSelector,\n\u001b[32m    133\u001b[39m )\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mindexes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    135\u001b[39m     CategoricalIndex,\n\u001b[32m    136\u001b[39m     Index,\n\u001b[32m   (...)\u001b[39m\u001b[32m    139\u001b[39m     default_index,\n\u001b[32m    140\u001b[39m )\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minternals\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mblocks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ensure_block_shape\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1176\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1147\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:690\u001b[39m, in \u001b[36m_load_unlocked\u001b[39m\u001b[34m(spec)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:936\u001b[39m, in \u001b[36mexec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1032\u001b[39m, in \u001b[36mget_code\u001b[39m\u001b[34m(self, fullname)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1130\u001b[39m, in \u001b[36mget_data\u001b[39m\u001b[34m(self, path)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "import seaborn as sns \n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression,Ridge,Lasso,ElasticNet\n",
    "from sklearn.metrics import r2_score \n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d069a225-bb70-45d7-83cf-be7b992d0b58",
   "metadata": {},
   "source": [
    "First, let's define some functions that will help us in the future analysis.\n",
    "\n",
    "Below function will calculate the $R^{2}$ on each feature given the input of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99353c36-0260-4ee8-a1eb-6fc622a488c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_R2_features(model,test=True): \n",
    "    #X: global  \n",
    "    features=list(X)\n",
    "    features.remove(\"three\")\n",
    "    \n",
    "    R_2_train=[]\n",
    "    R_2_test=[]\n",
    "\n",
    "    for feature in features:\n",
    "        model.fit(X_train[[feature]],y_train)\n",
    "        \n",
    "        R_2_test.append(model.score(X_test[[feature]],y_test))\n",
    "        R_2_train.append(model.score(X_train[[feature]],y_train))\n",
    "        \n",
    "    plt.bar(features,R_2_train,label=\"Train\")\n",
    "    plt.bar(features,R_2_test,label=\"Test\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.ylabel(\"$R^2$\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    print(\"Training R^2 mean value {} Testing R^2 mean value {} \".format(str(np.mean(R_2_train)),str(np.mean(R_2_test))) )\n",
    "    print(\"Training R^2 max value {} Testing R^2 max value {} \".format(str(np.max(R_2_train)),str(np.max(R_2_test))) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44a990a-c23a-423e-ac11-20ba5d1dad9e",
   "metadata": {},
   "source": [
    "Below function will plot the estimated coefficients for each feature and find $R^{2}$ on training and testing sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e72736-e748-42a7-bc9b-98603fa7bf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_coef(X,model,name=None):\n",
    "    \n",
    "\n",
    "    plt.bar(X.columns[2:],abs(model.coef_[2:]))\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.ylabel(\"$coefficients$\")\n",
    "    plt.title(name)\n",
    "    plt.show()\n",
    "    print(\"R^2 on training  data \",model.score(X_train, y_train))\n",
    "    print(\"R^2 on testing data \",model.score(X_test,y_test))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d4c778-462f-43e9-97cb-b00015d13a3b",
   "metadata": {},
   "source": [
    "Below function plots the distribution of two inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bbd96d-4e2b-4980-aa39-63e19db76ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def  plot_dis(y,yhat):\n",
    "    \n",
    "    plt.figure()\n",
    "    ax1 = sns.distplot(y, hist=False, color=\"r\", label=\"Actual Value\")\n",
    "    sns.distplot(yhat, hist=False, color=\"b\", label=\"Fitted Values\" , ax=ax1)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.title('Actual vs Fitted Values')\n",
    "    plt.xlabel('Price (in dollars)')\n",
    "    plt.ylabel('Proportion of Cars')\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3967f9-8e08-4306-84fc-16542e1f76ab",
   "metadata": {},
   "source": [
    "## **Reading and understanding our data**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b71c563-94d6-43fc-809b-7454eca904b8",
   "metadata": {},
   "source": [
    "For this lab, we will be using the car sales dataset, hosted on IBM Cloud object storage. The dataset contains all the information about cars, the name of the manufacturer, the year it was launched, all car technical parameters, and the sale price. This dataset has already been pre-cleaned and encoded (using one-hot and label encoders) in the Linear Regression Notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85a2304-e582-413c-8147-4e6fe99b97c1",
   "metadata": {},
   "source": [
    "Let's read the data into *pandas* data frame and look at the first 5 rows using the `head()` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c95e34-c508-4927-add1-5083a24911df",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML240EN-SkillsNetwork/labs/encoded_car_data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40dadcf-da42-49a6-9f46-45444002db2e",
   "metadata": {},
   "source": [
    "We can find more information about the features and types using the `info()`  method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2522b670-5846-445f-b623-ecc78713291d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9475961-3c81-42d0-b191-b10ee37165d1",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Let's first split our data into `X` features and `y` target.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eeba08e-fa2b-4cc9-82a3-552994353f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('price', axis=1)\n",
    "y = data.price"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b23b972-0520-4942-82db-36efb23bed9d",
   "metadata": {},
   "source": [
    "Now that we have split our data into training and testing sets, the training data is used for your model to recognize patterns using some criteria,the test data set it used to evaluate your model, as shown in the following image:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8819176-2cbe-4531-826e-dfbbfeb3858c",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML240EN-SkillsNetwork/images/trin-test.png\">\n",
    "</center>\n",
    "<center>source scikit-learn.org</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cae5be-dceb-4e9a-8901-9fc3611a268f",
   "metadata": {},
   "source": [
    "Now, we split our data, using <code>train_test_split</code> function, into the training and testing sets, allocating 30% of the data for testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0ec3c3-1608-414f-ab43-8d22bcf7feab",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.1, random_state=42)\n",
    "print(\"number of test samples :\", X_test.shape[0])\n",
    "print(\"number of training samples:\",X_train.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafc5b2d-514d-4cda-ad64-9c6d2d3cd0a6",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "In linear  regression we are trying to find the value of $\\textbf{w}$ that  minimizes the Mean Squared Error (MSE), we can represent this using the following expression:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92511afb-ba21-4acf-965c-486b86536f8c",
   "metadata": {},
   "source": [
    "$\\hat{\\textbf{w}}= \\underset{\\textbf{w}}{\\mathrm{argmin}} {\n",
    "||\\textbf{y}  - \\textbf{Xw} ||^2_2}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514fc635-7834-4019-82de-8e26279f644c",
   "metadata": {},
   "source": [
    "Where $\\textbf{y}$ is the target, $\\textbf{X}$ is the training set and $\\textbf{w}$ is the parameter weights. The resulting $\\hat{\\textbf{w}}$ is the best value to minimize the MSE, i.e., the distance between the target $\\textbf{y}$ and the estimate $\\textbf{Xw}$. We do this by fitting the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f1a49f-515a-4fb5-9878-78d2c439f34f",
   "metadata": {},
   "source": [
    "Let's create a <code>LinearRegression</code> object, called `lm`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79104d00-c874-467d-b813-254d85710ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20192a1c-5769-4348-a2eb-02e015fdc750",
   "metadata": {},
   "source": [
    "Now, let's fit the model with multiple features on our X_train and y_train data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d348cf-ea58-4741-adff-ad11424ee040",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c137b8a-6bd0-4702-91d9-e21c624f3bb0",
   "metadata": {},
   "source": [
    "We apply `predict(`) function on the testing data set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fbb51c-99ea-4538-aa3c-97758bf0286b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = lm.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b255ad-ddca-4ff8-abbf-ef7a1f79fc59",
   "metadata": {},
   "source": [
    "Let's calculate the `$R^2$` on both, training and testing data sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44f0ca0-a246-4f94-9681-afb8cfc504d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"R^2 on training  data \",lm.score(X_train, y_train))\n",
    "print(\"R^2 on testing data \",lm.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851f2511-558a-4ac7-8347-988f313bbb78",
   "metadata": {},
   "source": [
    "We can plot a distribution of the predicted values vs the actual values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23989f2e-15ea-45c6-8d4a-a520ada298f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dis(y_test,predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee1533c-cfd5-46ed-b0e8-d9c90de13a7e",
   "metadata": {},
   "source": [
    "We can view the estimated coefficients for the linear regression problem and drop the top two coefficients, as they are two large.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9e1aa2-62d8-4498-bd77-2456733c3436",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_coef(X,lm,name=\"Linear Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c23df8-47c5-4bc8-8aed-d6aeb23477ec",
   "metadata": {},
   "source": [
    "## Ridge Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa852d41-6416-48ed-9b35-c1a855d8cbf1",
   "metadata": {},
   "source": [
    "Let's review the Ridge Regression. Ridge Regression makes the prior assumption that our coefficients are normally distributed around zero. A regularization term, alpha, is added to the cost function. This forces the learning algorithm to not only fit the data but also keep the model weights as small as possible. The variance of the distribution is inversely proportional to the parameter alpha. This is also called the  L2 regularizer , as it adds a L2 penalty to the minimization term, as shown here:\n",
    "\n",
    "$\\hat{\\textbf{w}}= \\underset{\\textbf{w}}{\\mathrm{argmin}} {\n",
    "||\\textbf{y}  - \\textbf{Xw} ||^2_2+ \\alpha  ||\\textbf{w}||_2 }$\n",
    "\n",
    "We minimize the MSE, but we also penalize large weights by including their magnitude $||\\textbf{w}||\\_2$ in the minimization term. This additional minimization term makes the model less susceptible to noise and makes the weights smaller. Alpha controls the takeoff between MSE and penalization or regularization term and is chosen via cross-validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38f21ef-2f8e-4bef-85c4-1a75646bef52",
   "metadata": {},
   "source": [
    "Let's see  how the parameter alpha changes the model. Note, here our test data will be used as validation data. Also, the regularization term should only be added to the cost function during the training.\n",
    "\n",
    "Let's create a Ridge Regression object, setting the regularization parameter (alpha) to 0.01.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bbf666-39f1-4199-b542-a5dbdb1bfd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "rr = Ridge(alpha=0.01)\n",
    "rr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f3cd88-6506-4daf-9797-7b6fb5845ca2",
   "metadata": {},
   "source": [
    "Like regular regression, you can fit the model using the `fit()` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8155a8b6-4b7d-477d-a760-87f407e0aeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "rr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07a3e87-1bfd-487a-be64-6070ac37cda8",
   "metadata": {},
   "source": [
    "Similarly, you can obtain a prediction:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c0a01d-8462-45a5-b206-ea24866b4c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7837b6-2a01-46d5-b69f-b7a8ca2a7e71",
   "metadata": {},
   "source": [
    "We can calculate the $R^2$ on the training and testing data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062b2f37-744c-4478-92e1-3169259a709f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"R^2 on training  data \",rr.score(X_train, y_train))\n",
    "print(\"R^2 on testing data \",rr.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b6e257-4d2c-41cc-bb29-dd84b029e344",
   "metadata": {},
   "source": [
    "Now let's compare the Ridge Regression and the Linear Regression  models. The results on the $R^2$ are about the same, and the coefficients seem to be smaller.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1803b14a-d269-47fc-8c05-df178c650b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_coef(X,lm,name=\"Linear Regression\")\n",
    "plot_coef(X,rr,name=\"Ridge Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02292db-78c4-42eb-bc49-498d5f314eed",
   "metadata": {},
   "source": [
    "If we increase alpha, the coefficients get smaller, but the results are not as good as our previous value of alpha.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cca8a0-109a-4991-8cac-e901f7afcfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "rr = Ridge(alpha=1)\n",
    "rr.fit(X_train, y_train)\n",
    "plot_coef(X,rr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491c4ab1-7dd9-4437-a39b-88f6f0eef1d9",
   "metadata": {},
   "source": [
    "In general, we see that if we increase alpha, the coefficients get smaller, but the model performance relationship gets more complex. As a result, we use the validation data to select a value for alpha. Here, we plot the coefficients and $R^2$ of the test data on the vertical axes and alpha on the horizontal axis, as well the $R^2$ using the test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73647a64-449f-4af6-b488-3068e6a273e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [0.00001,0.0001,0.001,0.01,0.1,1,10,100]\n",
    "R_2=[]\n",
    "coefs = []\n",
    "for alpha in alphas:\n",
    "    ridge = Ridge(alpha=alpha)\n",
    "    ridge.fit(X_train, y_train)\n",
    "    coefs.append(abs(ridge.coef_))\n",
    "    R_2.append(ridge.score(X_test,y_test))\n",
    "\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.plot(alphas, coefs)\n",
    "ax.set_xscale(\"log\")\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"weights\")\n",
    "plt.title(\"Ridge coefficients as a function of the regularization (regularization path)\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.plot(alphas, R_2)\n",
    "ax.set_xscale(\"log\")\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"$R^2$\")\n",
    "plt.title(\"$R^2$ as a function of the regularization\")\n",
    "plt.show()          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0faca810-91bb-4a93-865a-c51fd786466b",
   "metadata": {},
   "source": [
    "As we increase alpha, the coefficients get smaller but the $R^2$ peaks when alpha is 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e11c20c-5389-4cba-960a-2e77debeb1c5",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "In this Exercise, plot the MSE as a function of alpha. What pattern do you notice?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e2d289-2728-457b-9c36-d5a66a33382d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code and run the cell\n",
    "alphas = [0.00001,0.0001,0.001,0.01,0.1,1,10]\n",
    "MEAN_SQE=[]\n",
    "\n",
    "for alpha in alphas:\n",
    "    ridge = Ridge(alpha=alpha)\n",
    "    ridge.fit(X_train, y_train)\n",
    "    MEAN_SQE.append(mean_squared_error(ridge.predict(X_test),y_test))\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.plot(alphas, MEAN_SQE)\n",
    "ax.set_xscale(\"log\")\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.title(\"$MSE$ as a function of the regularization\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1b404e-9650-45ea-b744-a2575cc2b3b9",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "A small alpha leads to over-fitting but as alpha gets larger the MSE decreases. When alpha gets too large the MSE increases leading to underfitting. The optimal point seems to be in the middle .\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd7218a-e407-48de-96da-c652a2694e43",
   "metadata": {},
   "source": [
    "## Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8796a5-7bb5-4ed8-82b0-a86016f14cda",
   "metadata": {},
   "source": [
    "We can also create a Pipeline object and apply a set of transforms sequentially. Then, we can apply Polynomial Features, perform data standardization then apply Ridge regression.  Data Pipelines simplify the steps of processing the data. We use the module `Pipeline` to create a pipeline. We also use `StandardScaler` step in our pipeline. Scaling our data is necessary step in Ridge regression as it will penalize features with a large magnitude.\n",
    "\n",
    "Now, we create a pipeline object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9ea873-5cec-4eb3-861c-5ac49bfd948c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Input=[ ('polynomial', PolynomialFeatures(include_bias=False,degree=2)),('ss',StandardScaler() ), ('model',Ridge(alpha=1))]\n",
    "pipe = Pipeline(Input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d245071a-e950-4445-8dfc-3a2d9836f384",
   "metadata": {},
   "source": [
    "We fit the object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b09502-cdb6-4327-b43e-3553a8268aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04daecd-8cda-4626-8b8b-f09f8a6cb5ff",
   "metadata": {},
   "source": [
    "We can calculate the score on the test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda94379-17d9-42a4-9df5-9969144527db",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted=pipe.predict(X_test)\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab7b4db-96f3-4a67-978e-3efd0a64d2d9",
   "metadata": {},
   "source": [
    "Looking for hyperparameters can get difficult with loops. The problem will get worse as we add more transforms such as polynomial transform. Therefore, we can use `GridSearchCV` to make things simpler.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bac321-ea5d-4370-85cc-aa382b2da45c",
   "metadata": {},
   "source": [
    "## GridSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e85012-703a-4106-8feb-7a7df0b163f3",
   "metadata": {},
   "source": [
    "To search for the best combination of hyperparameters we can create a  `GridSearchCV()` function as a dictionary of parameter values. The parameters of pipelines can be set by using the name of the key, separated by \"\\__\", then the parameter. Here, we look for different polynomial degrees and different values of alpha.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6c8565-6108-48b0-9979-ed6b369a2f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"polynomial__degree\": [1,2,3,4],\n",
    "    \"model__alpha\":[0.0001,0.001,0.01,0.1,1,10]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6822ede3-9876-4d20-bb7c-5d8646e4f233",
   "metadata": {},
   "source": [
    "Keys of the dictionary are the model \"key name \\__\" followed by the parameter as an attribute.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b726fb18-8255-4351-bd8f-a451b6ecc6a5",
   "metadata": {},
   "source": [
    "<b>polynomial\\_\\_degree</b>: is the degree of the polynomial; in this case 1, 2, 3, 4 and 5.\n",
    "\n",
    "<b>model\\_\\_alpha </b>: Regularization strength; must be a positive float.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d137ec57-6f5c-49d3-af1e-2e50e8fea79b",
   "metadata": {},
   "source": [
    "We create a `GridSearchCV` object and fit it. The method trains the model and the hyperparameters are selected via exhaustive search over the specified values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388e8425-5095-44c2-8d72-5bcaeb40a85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = GridSearchCV(pipe, param_grid, n_jobs=2)\n",
    "\n",
    "search.fit(X_train, y_train)\n",
    "search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c4fca8-8534-4868-9016-1a319cf10c2b",
   "metadata": {},
   "source": [
    "We can input the results into *pandas* `DataFrame()` as a dictionary with keys as column headers and values as columns and display the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53881c54-ef81-4085-8da1-b35881752643",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(search.cv_results_).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1663edb6-eba7-446e-bc58-ca9fa4ec0b9b",
   "metadata": {},
   "source": [
    "There are some other useful attributes:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe6a7ec-6779-4846-9d55-4f7c50542a51",
   "metadata": {},
   "source": [
    "`best_score_`: mean cross-validated score of the `best_estimator`.\n",
    "\n",
    "`best_params_dict`: parameter setting that gives the best results on the hold-out data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30851969-a844-41fa-b2d1-125547a97f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"best_score_: \",search.best_score_)\n",
    "print(\"best_params_: \",search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5e1793-082b-4ee5-b723-92bcb8646778",
   "metadata": {},
   "source": [
    "We can call `predict()` on the estimator with the best found parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658549f1-47e5-4030-afca-b92933417d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predict = search.predict(X_test)\n",
    "\n",
    "predict "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8611d6-b88a-47c2-8592-a815dc361988",
   "metadata": {},
   "source": [
    "We can find the best model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49972f60-570e-4de2-8318-f3e5e155f333",
   "metadata": {},
   "outputs": [],
   "source": [
    "best=search.best_estimator_\n",
    "best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0512a246-8123-4de3-9ffc-4fb080397488",
   "metadata": {},
   "source": [
    "As we can see from the above output, it is five degree polynomial with alpha value of 0.0001.\n",
    "Now, let's make a prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f9b119-b762-4b64-9cb2-4da27755f17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = best.predict(X_test)\n",
    "predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e09194-9576-453c-a87e-4711ea325a03",
   "metadata": {},
   "source": [
    "We can calculate the $R^2$ on the test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f04d65-c97f-4d54-badc-d3c5a9489e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "best.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38b8dd1-01d1-4462-81b0-c4f4a1373cf0",
   "metadata": {},
   "source": [
    "As we see, using Ridge Regression polynomial function works better than all other models. Finely, we can train our model on the entire data set!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef9d470-4b42-4b76-aee8-492387731ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "best.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0903bf83-9f01-4793-b698-5fc5be128298",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Perform grid search on the following features and plot the results by completing the following lines of code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76e1c21-4bd4-408e-8192-f6c68349c992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the cell\n",
    "columns=['wheelbase', 'curbweight', 'enginesize', 'boreratio', 'horsepower',\n",
    "       'carlength', 'carwidth', 'citympg']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751c9f9f-5a7a-4a01-9e3a-2611fa552c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code and run the cell\n",
    "for column in columns:\n",
    "    search.fit(X_train[[column]], y_train)\n",
    "    x=np.linspace(X_test[[column]].min(), X_test[[column]].max(),num=100)\n",
    "    plt.plot(x,search.predict(x.reshape(-1,1)),label=\"prediction\")\n",
    "    plt.plot(X_test[column],y_test,'ro',label=\"y\")\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d242a53d-18e4-4232-a3fd-b57cedc0d7b4",
   "metadata": {},
   "source": [
    "## Lasso Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a47934-f156-43c5-8130-5f8ad91fbac0",
   "metadata": {},
   "source": [
    "In this section, let's review the Lasso (Least Absolute Shrinkage and Selection Operator) Regression. Lasso Regression makes the prior assumption that our coefficients have Laplace (double-exponential) distribution around zero. The scale parameter of the distribution is inversely proportional to the parameter alpha. The main advantage of LASSO Regression is that many coefficients are set to zero, therefore they are not required. This has many advantages, one of them is that you may not need to collect and/or store all of the features. This may save resources. For example, if the feature was some medical test, you would no longer need to perform that test. Let's see how the parameter alpha changes the model.  We minimize the MSE, but we also penalize large weights by including their sum of absolute values $||\\textbf{w}||_1$ , symbolically:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6603e96f-62c1-4f07-bcf1-24c9ecb2ea66",
   "metadata": {},
   "source": [
    "$\\hat{\\textbf{w}}= \\underset{\\textbf{w}}{\\mathrm{argmin}} {\n",
    "||\\textbf{y}  - \\textbf{Xw} ||^2_2+ \\alpha  ||\\textbf{w}||_1 }$\n",
    "\n",
    "This regularization or penalty term makes many coefficients zero, making the model easy to understand and can also be used for feature selection. There are some drawbacks to this technique. It takes longer time to train and the solution may not be unique. Alpha controls the trade-off between MSE and penalization or regularization term and is chosen via cross-validation.  Let's see how the parameter alpha changes the model. Note, as before, our test data will be used as validation data. Let's create a Ridge Regression object, setting the regularization parameter (alpha) to 0.01.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af41560-fa49-4236-bf4c-c1a4fc6c1093",
   "metadata": {},
   "outputs": [],
   "source": [
    "la = Lasso(alpha=0.1)\n",
    "la.fit(X_train,y_train)\n",
    "la"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed796a27-97bb-4735-9848-cbf1146abe33",
   "metadata": {},
   "source": [
    "Let's make a prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2367a60-adf8-48a4-9efb-b88e26bd7bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = la.predict(X_test)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510fe36e-85e8-4f11-974e-f6a2ae226fcc",
   "metadata": {},
   "source": [
    "Let's calculate the $R^2$ on the training and testing data and see how it performs compared to the other methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ce6c74-f5b9-4a63-9848-0c252d250d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"R^2 on training  data \",lm.score(X_train, y_train))\n",
    "print(\"R^2 on testing data \",lm.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151a4202-0d8f-4b29-90ab-535b56a9bf0a",
   "metadata": {},
   "source": [
    "If we compare the Lasso Regression to the  Ridge Regression model we see that the results on the $R^2$ are slightly worse, but most of the coefficients are zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1272d6d8-1563-447f-b84e-c5a1557da289",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_coef(X,rr,name=\"Ridge Regression\")\n",
    "plot_coef(X,la,name=\"Lasso Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bf0a14-0366-4e60-be2d-2cdc819faef1",
   "metadata": {},
   "source": [
    "Similar to the Ridge Regression, if we increase the value of alpha, the coefficients will get smaller. Additionally, many coefficients become zero. Moreover, the model performance relationship becomes more complex. As a result, we use the validation data to select a value for alpha. Here, we plot the coefficients and $R^2$ of the test data on the vertical axes and alpha values on the horizontal axis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8ab332-af96-469b-af72-6d1e62688ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [0.00001,0.0001,0.001,0.01,0.1,1,10,100,1000]\n",
    "R_2=[]\n",
    "coefs = []\n",
    "for alpha in alphas:\n",
    "    la=Lasso(alpha=alpha)\n",
    "    \n",
    "    la.fit(X_train, y_train)\n",
    "    coefs.append(abs(la.coef_))\n",
    "    R_2.append(la.score(X_test,y_test))\n",
    "\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.plot(alphas, coefs)\n",
    "ax.set_xscale(\"log\")\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"weights\")\n",
    "plt.title(\"Ridge coefficients as a function of the regularization (regularization path)\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.plot(alphas, R_2)\n",
    "ax.set_xscale(\"log\")\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"$R^2$\")\n",
    "plt.title(\"$R^2$ as a function of the regularization\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06575219-6c87-4860-9c3e-88d1298cb8ef",
   "metadata": {},
   "source": [
    "## Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3789df-ce3e-4617-83c9-5b0e57fd3671",
   "metadata": {},
   "source": [
    "We can also create a Pipeline object and apply a set of transforms sequentially. Then, we can apply polynomial features, perform data standardization, then apply Lasso Regression.  We also use `StandardScaler` as a step in our pipeline. Scaling your data is necessary step in LASSO Regression, as it will penalize features with a large magnitudes.\n",
    "\n",
    "We start by creating a pipeline object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d98cb73-ef36-408a-8783-4a177c317d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "Input=[ ('polynomial', PolynomialFeatures(include_bias=False,degree=2)),('ss',StandardScaler() ), ('model',Lasso(alpha=1, tol = 0.2))]\n",
    "pipe = Pipeline(Input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29235fff-3966-49c0-8421-637d89836998",
   "metadata": {},
   "source": [
    "Then we fit the object, and make our predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3866ff-1992-44f6-8725-e269a99617ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train, y_train)\n",
    "pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a2110b-b886-4ea4-84dd-80daf960d559",
   "metadata": {},
   "source": [
    "We can calculate the $R^2$ on the training and testing data sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef3686b-81bf-415d-b453-a041e3ec3bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"R^2 on training  data \",pipe.score(X_train, y_train))\n",
    "print(\"R^2 on testing data \",pipe.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54db307d-0e40-4520-8c99-65a79878b325",
   "metadata": {},
   "source": [
    "As we see, some individual features perform similarly to using all the features (we removed the feature `three` ). Additionally, we see the smaller coefficients seem to correspond to a larger $R^{2}$, therefore  larger coefficients correspond to overfiting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291a0ea0-833b-499f-b5a3-3da30b417c44",
   "metadata": {},
   "source": [
    "## GridSearchCV\n",
    "\n",
    "To search for the best combination of hyperparameters, we can create a  `GridSearchCV()` function as a dictionary of parameter values. The parameters of pipelines can be set by using the name of the key, separated by \"\\__\", then the parameter. Here, we look for different polynomial degrees and different values of alpha.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cb9b68-7118-4cac-8050-6098b7302d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"polynomial__degree\": [ 1, 2,3,4,5],\n",
    "    \"model__alpha\":[0.0001,0.001,0.01,0.1,1,10]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f2a0a1-d4e4-447a-85cf-3cf11d16518c",
   "metadata": {},
   "source": [
    "To search for the best combination of hyperparameters, we create a  `GridSearchCV` object with a dictionary of parameter values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0367321-a64e-4378-921d-82e234ea5b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = GridSearchCV(pipe, param_grid, n_jobs=1)\n",
    "search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac2a3b4-c34f-44ff-b8ab-9ea38d9f5b11",
   "metadata": {},
   "source": [
    "Now, we can find the best model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e71bd7-f0a5-4ce6-bb81-bdced5bee95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best=search.best_estimator_\n",
    "best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0116f7d-6ab5-46dd-8c93-cc515cde8a9c",
   "metadata": {},
   "source": [
    "We can calculate the $R^2$ on the test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b75b61-e333-4a29-b452-be26070607ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "best.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39447c7-b7aa-4025-8823-a7be133ca659",
   "metadata": {},
   "source": [
    "## Elastic Net\n",
    "\n",
    "In this section, let's review the Elastic Net Regression. It combines L1 and L2 priors as regularizes or penalties. So, we can combine the two as follows:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9529af0f-f47c-4a39-9b64-9a2ad7fe0d59",
   "metadata": {},
   "source": [
    "$\\hat{\\textbf{w}}= \\underset{\\textbf{w}}{\\mathrm{argmin}} {\n",
    "||\\textbf{y}  - \\textbf{Xw} ||^2_2+ \\alpha  \\rho||\\textbf{w}||\\_1\n",
    "0.5 \\alpha (1 - \\rho)  ||\\textbf{w}||^2_2 } $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b2e920-be96-4713-b181-88b0dc426b59",
   "metadata": {},
   "source": [
    "Additionally to the alpha term ($\\alpha$), we have a mixing parameter, $\\rho$, such that 0 $\\le$ $\\rho$ $\\le$ 1. For $\\rho$=0, the penalty is an L2 regularization . For $\\rho=0$, it is L1 regularization; otherwise, it is a combination of L1 and L2. In *scikit-learn* the parameter is called  `l1_ratio`. Unlike the Ridge Regression, Elastic Net finds zero coefficients. In many cases Elastic Net performs better than Lasso, as it includes features that are correlated with one another. One drawback of the Elastic Net is you have two hyperparameters.  Lets create a model where `alpha=0.1`and `l1_ratio=0.5` and fit the data with this model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e26ac6b-2c40-4046-87c4-50f187fd2613",
   "metadata": {},
   "outputs": [],
   "source": [
    "enet = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "enet.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041543d3-8c38-4817-92c3-9e559ab72421",
   "metadata": {},
   "source": [
    "Let's make a prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f3038d-41a8-4251-8f0d-9864ef98f7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predicted=enet.predict(X_test)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d784df-7c9c-4e17-9560-c140f9fff0c5",
   "metadata": {},
   "source": [
    "Let's calculate the $R^2$ on the test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84ec830-6ca2-4b13-9801-d3bc73983815",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"R^2 on training  data \", enet.score(X_train, y_train))\n",
    "print(\"R^2 on testing data \", enet.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73d21d5-559f-49d1-b76c-1623e143c014",
   "metadata": {},
   "source": [
    "If we compare the Elastic Net to Lasso Regression and  Ridge Regression, we see the results on the $R^2$ are better than the Elastic Net and many of the coefficients are zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cf19e4-1a67-4bda-ad9e-02a2323cfb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_coef(X,la,name=\"Lasso Regression\")\n",
    "plot_coef(X,enet,name=\"Elastic net \")\n",
    "\n",
    "## graph that leads to error "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b256162d-db40-4532-b4df-a196a5ba19e0",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "Create and fit the Elastic Net model and the Ridge Regression models and plot the coefficients for both models using the `plot_coef()`function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808b82b3-9111-4808-98c4-6217bc8e02ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code and run the cell\n",
    "enet = ElasticNet(alpha=0.01, l1_ratio=0)\n",
    "enet.fit(X_train,y_train)\n",
    "rr = Ridge(alpha=0.01)\n",
    "rr.fit(X_train,y_train)\n",
    "plot_coef(X,rr,name=\"Ridge Regression\")\n",
    "\n",
    "plot_coef(X,enet,name=\"Elastic net l1_ratio=0 \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9535636e-f637-456b-863f-497e77de51bd",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "\n",
    "Create a Pipeline object, apply polynomial features (degree = 2), perform data standardization, then apply Elastic Net with `alpha=0.1` and  `l1_ratio=0.1` parameters. Fit the model using the training data, then calculate the $R^2$ on the training and testing data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787e9bec-ed0c-420a-97b0-6ed312eb24c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code and run the cell\n",
    "Input=[ ('polynomial', PolynomialFeatures(include_bias=False,degree=2)),('ss',StandardScaler() ), ('model',ElasticNet(alpha=0.1, l1_ratio=0.1))]\n",
    "pipe = Pipeline(Input)\n",
    "pipe.fit(X_train, y_train)\n",
    "print(\"R^2 on training  data \",pipe.score(X_train, y_train))\n",
    "print(\"R^2 on testing data \",pipe.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8771080-dee3-4638-aa33-f78a99aec770",
   "metadata": {},
   "source": [
    "## Exercise 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40971ea6-d882-40cd-bff4-d3e69343715f",
   "metadata": {},
   "source": [
    "Search for the best combination of  hyperparameters by creating  a  `GridSearchCV` object for Elastic Net Regression. Find the best parameter values using the pipeline object, as used in the above examples. Use`param_grid`, then find thee $R^2$ on the test data using the best estimator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5567e9d3-6c77-42b8-930d-cea1b38e68ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"polynomial__degree\": [ 1, 2,3,4,5],\n",
    "    \"model__alpha\":[0.0001,0.001,0.01,0.1,1,10],\n",
    "    \"model__l1_ratio\":[0.1,0.25,0.5,0.75,0.9]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d69d71e-f938-4c21-b0de-26ce008e24cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code and run the cell\n",
    "Input=[ ('polynomial', PolynomialFeatures(include_bias=False,degree=2)),('ss',StandardScaler() ), ('model',ElasticNet(tol = 0.2))]\n",
    "pipe = Pipeline(Input)\n",
    "search = GridSearchCV(pipe, param_grid, n_jobs=2)\n",
    "search.fit(X_test, y_test)\n",
    "best=search.best_estimator_\n",
    "best.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521ff4a5-631f-45d4-b67f-619b042ae6fd",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (Optional)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f5a7ce-ecdd-4b95-a222-f0564fd61ffa",
   "metadata": {},
   "source": [
    "In this example, we will explore Principal Component Analysis to reduce the dimensionality of our data.\n",
    "We will do so by creating a Pipeline object first, then applying standard scaling and performing PCA, and then applying Elastic Net Regularization with the following parametrs: `tol=0.2`, `alpha=0.1` and  `l1_ratio=0.1`.\n",
    "Finally, we will fit the model using the training data, then calculate the  $ð‘…^2$  on the training and testing data sets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17281067-c8b1-42b2-af0b-98e63626a053",
   "metadata": {},
   "source": [
    "Before adding PCA as a prep-processing step, we have to standardize our data. Scaling the features makes them have the same standard deviation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94065c63-e1e6-4b0f-ba83-5636aedf16bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train[:] = scaler.fit_transform(X_train)\n",
    "X_train.columns = [f'{c} (scaled)' for c in X_train.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180bb170-4b75-40bf-b5d1-4beaae545e30",
   "metadata": {},
   "source": [
    "Now, let's perform PCA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d59a36-611b-45c1-91c4-a3048a4f2e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3559f5-628f-4503-954b-dbf0b1f68d18",
   "metadata": {},
   "source": [
    "\n",
    "We can find the projection of the dataset onto the principal components, let's call it X_train_hat , this is our \"new\" dataset, it is the same shape as the original dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e7ac51-b927-4ec1-9441-7120c02c3e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_hat = pca.transform(X_train)\n",
    "print(X_train_hat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c60a92f-b6b2-4c25-a9a5-1e98971a3ab2",
   "metadata": {},
   "source": [
    "Let's look at the new dataset as a dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cbc658-e407-4573-aeda-31be47660020",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_hat_PCA = pd.DataFrame(columns=[f'Projection  on Component {i+1}' for i in range(len(X_train.columns))], data=X_train_hat)\n",
    "X_train_hat_PCA.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1d3943-08ee-4fc3-8929-6c986d67301d",
   "metadata": {},
   "source": [
    "Now, let's see how much variance can be explained using these principal components (PCs).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed91591-28f3-49af-aa32-e9dac6a04703",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pca.explained_variance_ratio_)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.title(\"Component-wise variance and cumulative explained variance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96fc778-2a85-483e-bfeb-eb884234ce3b",
   "metadata": {},
   "source": [
    "In the graph above, the component-wise variance is depicted by the blue line, and the cumulative explained variance is explained by the orange line. We are able to explain \\~100% of the variance using just the first 20 PCs. Let's filter our dataset down to these 20 PCs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7118e540-6242-420d-86a1-13ec8f6f5c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 20\n",
    "X_train_hat_PCA = X_train_hat_PCA.iloc[:, :N]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9a5f4f-744b-437e-a880-ca16d50d413d",
   "metadata": {},
   "source": [
    "Let's create an Elastic Net model where `tol=0.2`,  `alpha=0.1` and  `l1_ratio=0.1` and fit the data with this model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e915d088-fb90-4c32-b233-55b8f1c4bd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "enet = ElasticNet(tol = 0.2, alpha=100, l1_ratio=0.75)\n",
    "enet.fit(X_train_hat_PCA, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea012ab-3f4d-445e-8371-c87e97ee17ae",
   "metadata": {},
   "source": [
    "## Exercise 6 (Optional)\n",
    "\n",
    "In this Exercise, create a Pipeline object, apply standard scaling, perform PCA and then finally fit an Elastic Net with `tol=0.2`, `alpha=0.1` and  `l1_ratio=0.1` parameters. Calculate the scores, $R^2$, on the training and testing data sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e87488-4ac3-416b-98a3-7d9db6ddc59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code and run the cell\n",
    "Input=[ ('scaler', StandardScaler()), ('pca', PCA(n_components = N)), ('model', ElasticNet(tol =0.2, alpha=0.1, l1_ratio=0.1))]\n",
    "pipe = Pipeline(Input)\n",
    "pipe.fit(X_train, y_train)\n",
    "print(\"R^2 on training  data \", pipe.score(X_train, y_train))\n",
    "print(\"R^2 on testing data \", pipe.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd490be-e940-4854-a991-eba68a5b4afa",
   "metadata": {},
   "source": [
    "# Congratulations! - You have completed the lab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6965994-8c61-4771-a8a2-36f96c7ce0ed",
   "metadata": {},
   "source": [
    "## Author\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b9639b-0340-4e3c-ba4b-0fcf646ca19b",
   "metadata": {},
   "source": [
    "<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML0101ENSkillsNetwork20718538-2021-01-01\" target=\"_blank\">Joseph Santarcangelo</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba72e2b-7485-4444-aad3-cbac30622475",
   "metadata": {},
   "source": [
    "### Other Contributors\n",
    "\n",
    "[Svitlana Kramar](https://www.linkedin.com/in/svitlana-kramar?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML240ENSkillsNetwork34171862-2022-01-01)\n",
    "\n",
    "[Kopal Garg](https://www.linkedin.com/in/gargkopal/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML240ENSkillsNetwork34171862-2022-01-01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17dccc4-a2a9-4457-880f-d222dfa685ca",
   "metadata": {},
   "source": [
    "<!--## Change Log\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c201bf78-263a-4859-95dd-d19d6976292d",
   "metadata": {},
   "source": [
    "<!--| Date (YYYY-MM-DD) | Version | Changed By  | Change Description                               |\n",
    "| ----------------- | ------- | ----------- | ------------------------------------------------ |\n",
    "| 2022-05-02        | 0.1     | Svitlana K. | Reviewed and fixed minor code and grammar errors |\n",
    "| 2022-05-10        | 0.2     | Kopal G.    | Added the PCA example                            |-->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "prev_pub_hash": "48fa6f6e37eeb4a01e290d8fba5ac6627acf2ebd0ac7ab5ee8bcaec8180996d8"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
